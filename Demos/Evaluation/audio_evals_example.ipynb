{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52441510",
   "metadata": {},
   "source": [
    "# Azure OpenAI Audio Evaluation Demo\n",
    "\n",
    "A clean demonstration of Azure OpenAI audio evaluation workflow.\n",
    "\n",
    "## Prerequisites\n",
    "- Azure OpenAI service with audio model deployment\n",
    "- API key configured in `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from scripts.eval_utils import AsyncEvalClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup evaluation client\n",
    "client = AsyncEvalClient()\n",
    "print(\"ðŸŽ‰ Azure OpenAI Evaluation Client ready!\")\n",
    "\n",
    "AUDIO_MODEL_DEPLOYMENT_NAME = \"gpt-4o-audio-preview\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e56aa35",
   "metadata": {},
   "source": [
    "## Load and Create Audio Dataset\n",
    "\n",
    "Load audio samples from HuggingFace and create an evaluation dataset file in JSONL format. The function will:\n",
    "- Download the audio dataset from hugging face\n",
    "- Convert audio to base64-encoded WAV format\n",
    "- Prepare the file for evaluation.\n",
    "- Display first 3 rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbfa678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.audio_utils import load_and_create_audio_dataset, display_items\n",
    "\n",
    "# Load dataset from Hugging Face and create the audio dataset for evaluation\n",
    "load_and_create_audio_dataset(\"AbstractTTS/CREMA-D\")\n",
    "\n",
    "# Display the created evaluation file\n",
    "display_items(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617fbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the evaluation file to Azure OpenAI\n",
    "eval_file_id = await client.upload_file(\n",
    "    file_name=\"audio_emotion_evaluation.jsonl\",\n",
    "    file_path=\"./data/audio_emotion_evaluation.jsonl\")\n",
    "print(f\"âœ… Eval file ID: {eval_file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a4aaf",
   "metadata": {},
   "source": [
    "## Upload Evaluation Dataset\n",
    "\n",
    "Upload the prepared audio evaluation file to your Azure OpenAI account. This file contains base64-encoded audio samples with expected emotions that will be used as the data source for running evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42750a",
   "metadata": {},
   "source": [
    "## Initialize Azure OpenAI Client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40282210",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = {\n",
    "      \"type\": \"score_model\",\n",
    "      \"name\": \"Tone/Emotion Grader\",\n",
    "      \"model\": AUDIO_MODEL_DEPLOYMENT_NAME,\n",
    "      \"input\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a helpful assistant that evaluates audio clips to judge whether they match a provided {{item.expected_emotion}}. The audio clip is the model''s prediction of emotion. Result must be a float in [0.0, 1.0] similarity to {{item.expected_emotion}}, where 1.0 means the speakerâ€™s tone exactly matches the expected emotion and 0.0 means it does not match at all. Do not return other text.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"input_audio\",\n",
    "              \"input_audio\": {\n",
    "                \"data\": \"{{ sample.output_audio.data }}\",\n",
    "                \"format\": \"wav\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"range\": [\n",
    "        0,\n",
    "        1\n",
    "      ],\n",
    "      \"pass_threshold\": 0.5\n",
    "    }\n",
    "\n",
    "eval_id = await client.create_eval_sdk(\n",
    "    name=\"Audio Emotion Evaluation\",\n",
    "    testing_criteria=[score_model],\n",
    "    data_source_config={\n",
    "    \"type\": \"custom\",\n",
    "    \"item_schema\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"audio_data\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"Base64-encoded WAV audio data.\"\n",
    "        },\n",
    "        \"expected_emotion\": {\n",
    "          \"type\": \"string\",\n",
    "          \"description\": \"The expected primary emotion in the audio.\"\n",
    "        }\n",
    "      },\n",
    "      \"required\": [\n",
    "        \"audio_data\",\n",
    "        \"expected_emotion\"\n",
    "      ]\n",
    "    },\n",
    "    \"include_sample_schema\": True,\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679ed84",
   "metadata": {},
   "source": [
    "## Create Evaluation with Audio Emotion Grader\n",
    "\n",
    "Define a score model grader that evaluates the emotional tone of the model's audio responses. The grader:\n",
    "- Listens to the model's audio output\n",
    "- Compares it against the expected emotion from the dataset\n",
    "- Returns a similarity score from 0.0 (no match) to 1.0 (exact match)\n",
    "- Sets a passing threshold of 0.5 for acceptable emotion matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2e275",
   "metadata": {},
   "source": [
    "## Run the Evaluation\n",
    "\n",
    "Configure the data source and input messages for the evaluation run. This defines:\n",
    "- The audio model to use for emotion identification\n",
    "- System prompt instructing the model to analyze audio emotions\n",
    "- User instructions to identify emotions from the audio input\n",
    "- How to format multi-modal messages with text and audio\n",
    "- Temperature and modality settings (text and audio output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0525fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = {\n",
    "    \"type\": \"completions\",\n",
    "    \"model\": AUDIO_MODEL_DEPLOYMENT_NAME,\n",
    "    \"sampling_params\": {\n",
    "      \"temperature\": 0.8\n",
    "    },\n",
    "    \"modalities\": [\n",
    "      \"text\",\n",
    "      \"audio\"\n",
    "    ],\n",
    "    \"source\": {\n",
    "      \"type\": \"file_id\",\n",
    "      \"id\": eval_file_id\n",
    "    },\n",
    "    \"input_messages\": {\n",
    "      \"type\": \"template\",\n",
    "      \"template\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are a assistant that tells the emotion of audio input. You will be given an audio input.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"type\": \"message\",\n",
    "          \"content\": {\n",
    "            \"type\": \"input_text\",\n",
    "            \"text\": \"Listen to the audio and identify the primary emotion. Respond with exactly one word from: anger, fear, disgust, happy, sad.\"\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"type\": \"message\",\n",
    "          \"content\": {\n",
    "            \"type\": \"input_audio\",\n",
    "            \"input_audio\": {\n",
    "              \"data\": \"{{item.audio_data}}\",\n",
    "              \"format\": \"wav\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "}\n",
    "\n",
    "run = await client.create_eval_run_sdk(eval_id, \"Audio Emotion Evaluation\", data_source)\n",
    "run_id = run['id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94651c",
   "metadata": {},
   "source": [
    "## Monitor and Display Results\n",
    "\n",
    "Poll the evaluation run status until completion, then retrieve and display the results in a DataFrame showing:\n",
    "- Item IDs\n",
    "- Grading results (pass/fail status based on emotion matching)\n",
    "- Expected emotions from the dataset\n",
    "- Model's identified emotions from the audio transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a068a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "\n",
    "while True:\n",
    "    run = await client.get_eval_run_sdk(eval_id=eval_id, run_id=run_id)\n",
    "    if run['status'] == \"completed\":\n",
    "        output_items_response = await client.get_eval_run_output_items_sdk(\n",
    "            eval_id=eval_id, run_id=run_id)\n",
    "\n",
    "        # Get the actual list of items from the response object\n",
    "        output_items = output_items_response.data if hasattr(output_items_response, 'data') else output_items_response\n",
    "\n",
    "        # Create DataFrame with safe access to nested fields\n",
    "        df_data = {\n",
    "            \"id\": [],\n",
    "            \"grading_results\": [],\n",
    "            \"expected_emotion\": [],\n",
    "            \"audio_output\": []\n",
    "        }\n",
    "\n",
    "        for item in output_items:\n",
    "            # Convert Pydantic model to dict if needed\n",
    "            item_dict = item.model_dump() if hasattr(item, 'model_dump') else item\n",
    "            \n",
    "            df_data[\"id\"].append(item_dict.get(\"id\", \"N/A\"))\n",
    "            df_data[\"grading_results\"].append(item_dict.get(\"status\", \"N/A\"))\n",
    "            \n",
    "            # Safely get expected emotion\n",
    "            datasource_item = item_dict.get('datasource_item', {})\n",
    "            df_data[\"expected_emotion\"].append(datasource_item.get(\"expected_emotion\", \"N/A\"))\n",
    "            \n",
    "            # Check if audio output exists\n",
    "            sample = item_dict.get(\"sample\", {})\n",
    "            output = sample.get(\"output\", {})\n",
    "            output_transcript = output[0].get(\"content\")\n",
    "            df_data[\"audio_output\"].append(output_transcript)\n",
    "\n",
    "        df = pd.DataFrame(df_data)\n",
    "        display(df)\n",
    "        break\n",
    "    if run['status'] == \"failed\":\n",
    "        print(\"Evaluation run failed:\")\n",
    "        print(run.get('error', 'Unknown error'))\n",
    "        break\n",
    "    print(f\"Status: {run['status']}. Waiting...\")\n",
    "    await asyncio.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "eval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
