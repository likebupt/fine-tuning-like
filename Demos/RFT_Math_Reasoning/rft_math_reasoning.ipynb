{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Fine-Tuning with OpenR1-Math-220k Dataset\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models using **Reinforcement Fine-Tuning (RFT)** with the OpenR1-Math-220k dataset - a collection of 220,000 advanced mathematical reasoning problems with verified step-by-step solutions.\n",
    "\n",
    "---\n",
    "\n",
    "##  Agenda\n",
    "\n",
    "1. **Dataset Overview & Preparation**\n",
    "2. **Environment Setup**\n",
    "3. **Data Validation & Exploration**\n",
    "4. **Base Model Evaluation**\n",
    "5. **Mathematical Grader Setup**\n",
    "6. **RFT Training Configuration**\n",
    "7. **Launch Fine-Tuning Job**\n",
    "8. **Monitor Training Progress**\n",
    "9. **Deploy & Test Fine-Tuned Model**\n",
    "10. **Evaluation & Comparison**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Dataset Overview\n",
    "\n",
    "**OpenR1-Math-220k** contains:\n",
    "- **220,000** mathematical reasoning problems\n",
    "- **2-4 verified reasoning traces** per problem\n",
    "- Problems from college-level and competition mathematics\n",
    "- Solutions with **detailed step-by-step reasoning**\n",
    "- Final answers in `\\boxed{}` format\n",
    "- **88%** verified using Math Verify, **12%** using Llama-3.3-70B-Instruct\n",
    "\n",
    "**Problem Domains:**\n",
    "- Algebra and polynomial factorization\n",
    "- Calculus and optimization\n",
    "- Probability theory\n",
    "- Geometry and trigonometry\n",
    "- Number theory and combinatorics\n",
    "- Linear algebra\n",
    "- Complex numbers and abstract mathematics\n",
    "\n",
    "**Why RFT?**\n",
    "- Multiple valid solution paths exist\n",
    "- Correctness can be verified automatically\n",
    "- Model learns from quality reasoning patterns\n",
    "- Better than SFT for multi-path reasoning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import random\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\" Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Configuration\n",
    "project_endpoint = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "aoai_account = os.getenv(\"AZURE_AOAI_ACCOUNT\")\n",
    "model_name = os.getenv(\"MODEL_NAME\", \"gpt-4o\")  # Base model for RFT\n",
    "\n",
    "# Azure OpenAI for evaluation\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Project Endpoint: {project_endpoint}\")\n",
    "print(f\"  Base Model: {model_name}\")\n",
    "print(f\"  Deployment: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure AI Project Client\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_endpoint,\n",
    "    credential=credential,\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group_name=resource_group\n",
    ")\n",
    "\n",
    "print(\" Azure AI Project Client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Validation & Exploration\n",
    "\n",
    "Before training, let's examine our prepared training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if JSONL files exist\n",
    "training_file = Path(\"training.jsonl\")\n",
    "validation_file = Path(\"validation.jsonl\")\n",
    "\n",
    "if not training_file.exists() or not validation_file.exists():\n",
    "    print(\" Training or validation files not found!\")\n",
    "    print(\"\\n Please run the data preparation script first:\")\n",
    "    print(\"   python scripts/prepare_data.py --input_dir ./training_data --output_dir ./\")\n",
    "else:\n",
    "    # Count examples\n",
    "    with open(training_file, 'r', encoding='utf-8') as f:\n",
    "        train_count = sum(1 for _ in f)\n",
    "    \n",
    "    with open(validation_file, 'r', encoding='utf-8') as f:\n",
    "        val_count = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\" Dataset files found:\")\n",
    "    print(f\"   Training examples: {train_count:,}\")\n",
    "    print(f\"   Validation examples: {val_count:,}\")\n",
    "    print(f\"   Total: {train_count + val_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sample training examples\n",
    "print(\" Sample Training Examples:\\n\" + \"=\"*80)\n",
    "\n",
    "with open(training_file, 'r', encoding='utf-8') as f:\n",
    "    # Show 2 random examples\n",
    "    lines = f.readlines()\n",
    "    samples = random.sample(lines, min(2, len(lines)))\n",
    "    \n",
    "    for i, line in enumerate(samples, 1):\n",
    "        example = json.loads(line)\n",
    "        messages = example['messages']\n",
    "        \n",
    "        print(f\"\\n Example {i}:\")\n",
    "        print(f\"\\nProblem:\\n{messages[1]['content'][:300]}...\")\n",
    "        print(f\"\\nSolution Preview:\\n{messages[2]['content'][:500]}...\")\n",
    "        print(f\"\\nSolution Length: {len(messages[2]['content'])} characters\")\n",
    "        print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Base Model Evaluation\n",
    "\n",
    "Test the base model's performance on a few mathematical problems before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat client for evaluation\n",
    "chat_client = ChatCompletionsClient(\n",
    "    endpoint=azure_openai_endpoint,\n",
    "    credential=AzureKeyCredential(azure_openai_key)\n",
    ")\n",
    "\n",
    "# System prompt for mathematical reasoning\n",
    "math_system_prompt = (\n",
    "    \"You are a mathematical reasoning expert. Solve problems with detailed \"\n",
    "    \"step-by-step thinking and provide final answers in \\\\boxed{} format. \"\n",
    "    \"Show all intermediate calculations and explain your reasoning clearly.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test base model on a sample problem\n",
    "def test_base_model(problem, max_tokens=4000):\n",
    "    \"\"\"Test base model on a mathematical problem.\"\"\"\n",
    "    response = chat_client.complete(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            SystemMessage(content=math_system_prompt),\n",
    "            UserMessage(content=problem)\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Get a test problem\n",
    "with open(validation_file, 'r', encoding='utf-8') as f:\n",
    "    test_example = json.loads(f.readline())\n",
    "    test_problem = test_example['messages'][1]['content']\n",
    "    ground_truth = test_example['messages'][2]['content']\n",
    "\n",
    "print(\" Testing Base Model:\\n\" + \"=\"*80)\n",
    "print(f\"\\nProblem:\\n{test_problem}\\n\")\n",
    "\n",
    "base_response = test_base_model(test_problem)\n",
    "\n",
    "print(f\"\\nBase Model Response:\\n{base_response}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGround Truth (first 500 chars):\\n{ground_truth[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Mathematical Grader Setup\n",
    "\n",
    "For RFT, we need a grading function that evaluates the quality of mathematical reasoning. This grader will:\n",
    "1. Check if the answer is in the correct `\\boxed{}` format\n",
    "2. Extract and compare the final answer\n",
    "3. Evaluate reasoning quality (length, structure, completeness)\n",
    "4. Assign a reward score (0.0 to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_answer(text):\n",
    "    \"\"\"Extract answer from \\boxed{} notation.\"\"\"\n",
    "    pattern = r'\\\\boxed\\{([^}]+)\\}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[-1] if matches else None\n",
    "\n",
    "def grade_mathematical_solution(solution, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Grade a mathematical solution based on:\n",
    "    - Format compliance (has \\boxed{} answer)\n",
    "    - Reasoning quality (length, structure)\n",
    "    - Answer correctness (if ground truth provided)\n",
    "    \n",
    "    Returns score between 0.0 and 1.0\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Check for boxed answer (30% of score)\n",
    "    predicted_answer = extract_boxed_answer(solution)\n",
    "    if predicted_answer:\n",
    "        score += 0.3\n",
    "    else:\n",
    "        return 0.1  # Minimal score if no answer provided\n",
    "    \n",
    "    # Check reasoning length (20% of score)\n",
    "    # Good solutions are typically 1000-8000 tokens\n",
    "    solution_length = len(solution)\n",
    "    if solution_length > 500:\n",
    "        score += 0.2\n",
    "    elif solution_length > 200:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Check for step-by-step reasoning indicators (20% of score)\n",
    "    step_indicators = ['step', 'first', 'next', 'then', 'therefore', 'thus', 'hence']\n",
    "    step_count = sum(1 for indicator in step_indicators if indicator.lower() in solution.lower())\n",
    "    if step_count >= 3:\n",
    "        score += 0.2\n",
    "    elif step_count >= 1:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Check answer correctness if ground truth provided (30% of score)\n",
    "    if ground_truth:\n",
    "        gt_answer = extract_boxed_answer(ground_truth)\n",
    "        if gt_answer and predicted_answer:\n",
    "            # Normalize answers for comparison\n",
    "            pred_normalized = predicted_answer.strip().lower().replace(' ', '')\n",
    "            gt_normalized = gt_answer.strip().lower().replace(' ', '')\n",
    "            \n",
    "            if pred_normalized == gt_normalized:\n",
    "                score += 0.3\n",
    "            elif pred_normalized in gt_normalized or gt_normalized in pred_normalized:\n",
    "                score += 0.15\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "# Test the grader\n",
    "test_solution = \"\"\"Let's solve this step by step.\n",
    "First, we analyze the problem.\n",
    "Next, we apply the formula.\n",
    "Therefore, the answer is \\\\boxed{42}.\"\"\"\n",
    "\n",
    "print(f\"Test grader score: {grade_mathematical_solution(test_solution):.2f}\")\n",
    "print(\" Grading function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Upload Training Data\n",
    "\n",
    "Upload the training and validation datasets to Azure AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training file\n",
    "print(\" Uploading training data...\")\n",
    "with open(training_file, \"rb\") as f:\n",
    "    train_data = project_client.upload_file(f)\n",
    "print(f\" Training data uploaded: {train_data.id}\")\n",
    "\n",
    "# Upload validation file\n",
    "print(\" Uploading validation data...\")\n",
    "with open(validation_file, \"rb\") as f:\n",
    "    val_data = project_client.upload_file(f)\n",
    "print(f\" Validation data uploaded: {val_data.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Configure RFT Training\n",
    "\n",
    "Set up the fine-tuning configuration for Reinforcement Fine-Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFT Training Configuration\n",
    "rft_config = {\n",
    "    \"model\": model_name,\n",
    "    \"training_file\": train_data.id,\n",
    "    \"validation_file\": val_data.id,\n",
    "    \"hyperparameters\": {\n",
    "        \"n_epochs\": 2,  # 2-3 epochs for mathematical reasoning\n",
    "        \"batch_size\": 1,  # Small batch for long reasoning chains\n",
    "        \"learning_rate_multiplier\": 0.5  # Conservative LR to preserve reasoning\n",
    "    },\n",
    "    \"suffix\": \"math-reasoning-rft\",  # Model name suffix\n",
    "}\n",
    "\n",
    "print(\" RFT Configuration:\")\n",
    "print(f\"  Model: {rft_config['model']}\")\n",
    "print(f\"  Epochs: {rft_config['hyperparameters']['n_epochs']}\")\n",
    "print(f\"  Batch Size: {rft_config['hyperparameters']['batch_size']}\")\n",
    "print(f\"  Learning Rate Multiplier: {rft_config['hyperparameters']['learning_rate_multiplier']}\")\n",
    "print(f\"  Model Suffix: {rft_config['suffix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Launch Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fine-tuning job\n",
    "print(\" Launching RFT fine-tuning job...\")\n",
    "\n",
    "fine_tune_job = project_client.inference.create_fine_tuning_job(\n",
    "    model=rft_config[\"model\"],\n",
    "    training_file=rft_config[\"training_file\"],\n",
    "    validation_file=rft_config[\"validation_file\"],\n",
    "    hyperparameters=rft_config[\"hyperparameters\"],\n",
    "    suffix=rft_config[\"suffix\"]\n",
    ")\n",
    "\n",
    "job_id = fine_tune_job.id\n",
    "print(f\"\\n Fine-tuning job created: {job_id}\")\n",
    "print(f\"   Status: {fine_tune_job.status}\")\n",
    "print(f\"   Model: {fine_tune_job.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Monitor job status\n",
    "print(\" Monitoring training progress...\\n\")\n",
    "\n",
    "while True:\n",
    "    job_status = project_client.inference.get_fine_tuning_job(job_id)\n",
    "    status = job_status.status\n",
    "    \n",
    "    print(f\"Status: {status} - {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    if status in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "        break\n",
    "    \n",
    "    time.sleep(60)  # Check every minute\n",
    "\n",
    "if status == \"succeeded\":\n",
    "    print(f\"\\n Fine-tuning completed successfully!\")\n",
    "    print(f\"   Fine-tuned model: {job_status.fine_tuned_model}\")\n",
    "    fine_tuned_model = job_status.fine_tuned_model\n",
    "else:\n",
    "    print(f\"\\n Fine-tuning {status}\")\n",
    "    if hasattr(job_status, 'error'):\n",
    "        print(f\"   Error: {job_status.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deploy Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fine-tuned model\n",
    "print(\" Deploying fine-tuned model...\")\n",
    "\n",
    "deployment = project_client.inference.create_deployment(\n",
    "    model=fine_tuned_model,\n",
    "    name=f\"math-reasoning-{int(time.time())}\"\n",
    ")\n",
    "\n",
    "deployment_name_ft = deployment.name\n",
    "print(f\"\\n Model deployed: {deployment_name_ft}\")\n",
    "print(f\"   Endpoint: {deployment.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Test Fine-Tuned Model\n",
    "\n",
    "Compare the fine-tuned model's performance with the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fine-tuned model on the same problem\n",
    "def test_finetuned_model(problem, deployment_name, max_tokens=4000):\n",
    "    \"\"\"Test fine-tuned model on a mathematical problem.\"\"\"\n",
    "    response = chat_client.complete(\n",
    "        model=deployment_name,\n",
    "        messages=[\n",
    "            SystemMessage(content=math_system_prompt),\n",
    "            UserMessage(content=problem)\n",
    "        ],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\" Testing Fine-Tuned Model:\\n\" + \"=\"*80)\n",
    "print(f\"\\nProblem:\\n{test_problem}\\n\")\n",
    "\n",
    "finetuned_response = test_finetuned_model(test_problem, deployment_name_ft)\n",
    "\n",
    "print(f\"\\nFine-Tuned Model Response:\\n{finetuned_response}\\n\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Evaluation & Comparison\n",
    "\n",
    "Evaluate both models on multiple test problems and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on multiple test problems\n",
    "num_test_problems = 5\n",
    "\n",
    "print(f\" Evaluating models on {num_test_problems} problems...\\n\")\n",
    "\n",
    "base_scores = []\n",
    "ft_scores = []\n",
    "\n",
    "with open(validation_file, 'r', encoding='utf-8') as f:\n",
    "    test_examples = [json.loads(line) for line in list(f)[:num_test_problems]]\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    problem = example['messages'][1]['content']\n",
    "    ground_truth = example['messages'][2]['content']\n",
    "    \n",
    "    print(f\"\\n Problem {i}/{num_test_problems}\")\n",
    "    print(f\"Problem: {problem[:100]}...\")\n",
    "    \n",
    "    # Test base model\n",
    "    base_solution = test_base_model(problem)\n",
    "    base_score = grade_mathematical_solution(base_solution, ground_truth)\n",
    "    base_scores.append(base_score)\n",
    "    print(f\"  Base model score: {base_score:.2f}\")\n",
    "    \n",
    "    # Test fine-tuned model\n",
    "    ft_solution = test_finetuned_model(problem, deployment_name_ft)\n",
    "    ft_score = grade_mathematical_solution(ft_solution, ground_truth)\n",
    "    ft_scores.append(ft_score)\n",
    "    print(f\"  Fine-tuned model score: {ft_score:.2f}\")\n",
    "    print(f\"  Improvement: {(ft_score - base_score):.2f}\")\n",
    "\n",
    "# Summary\n",
    "avg_base = sum(base_scores) / len(base_scores)\n",
    "avg_ft = sum(ft_scores) / len(ft_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Average Base Model Score: {avg_base:.3f}\")\n",
    "print(f\"Average Fine-Tuned Model Score: {avg_ft:.3f}\")\n",
    "print(f\"Average Improvement: {(avg_ft - avg_base):.3f} ({((avg_ft - avg_base) / avg_base * 100):.1f}%)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Key Takeaways\n",
    "\n",
    "After completing this RFT fine-tuning cookbook, you should observe:\n",
    "\n",
    " **Improved Reasoning Quality**\n",
    "- More detailed step-by-step explanations\n",
    "- Better structured mathematical arguments\n",
    "- Clearer intermediate calculations\n",
    "\n",
    " **Better Format Compliance**\n",
    "- Consistent use of `\\boxed{}` for final answers\n",
    "- Proper mathematical notation\n",
    "- Well-organized solution structure\n",
    "\n",
    " **Enhanced Problem-Solving**\n",
    "- Higher accuracy on complex problems\n",
    "- Better handling of multi-step reasoning\n",
    "- Improved performance across different mathematical domains\n",
    "\n",
    " **RFT Advantages**\n",
    "- Model learns from multiple solution paths\n",
    "- Reward-based learning encourages quality reasoning\n",
    "- Better generalization than supervised fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "##  Cleanup (Optional)\n",
    "\n",
    "Remember to delete resources when you're done to avoid charges:\n",
    "- Fine-tuning jobs\n",
    "- Deployed models\n",
    "- Uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup code (uncomment to use)\n",
    "# print(\" Cleaning up resources...\")\n",
    "\n",
    "# # Delete deployment\n",
    "# project_client.inference.delete_deployment(deployment_name_ft)\n",
    "# print(f\" Deleted deployment: {deployment_name_ft}\")\n",
    "\n",
    "# # Delete uploaded files\n",
    "# project_client.delete_file(train_data.id)\n",
    "# project_client.delete_file(val_data.id)\n",
    "# print(\" Deleted uploaded files\")\n",
    "\n",
    "# print(\"\\n Cleanup complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
